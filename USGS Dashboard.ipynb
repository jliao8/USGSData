{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a68205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from xml.etree import ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED\n",
    "from time import perf_counter\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ccff5fb-450a-40a5-89a2-96496a7e438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea from https://stackoverflow.com/a/76596818\n",
    "def process_quakeml(responseContent):\n",
    "    path = []\n",
    "    rows = [] # no outer dict because some events don't have some tag (different columns -> rows don't match)\n",
    "    parser = ET.XMLPullParser(events=(\"start\",\"end\"))\n",
    "    parser.feed(responseContent)\n",
    "    \n",
    "    for event, element in parser.read_events():\n",
    "        key = element.tag[element.tag.rfind(\"}\")+1:] # gets rid of namespace\n",
    "        exclude_tags = [\"quakeml\", \"eventParameters\", \"event\"] # useless info\n",
    "        if event == \"start\" and key == \"event\":\n",
    "            row = {} # function scope\n",
    "            path.append(key)\n",
    "        elif event == \"end\" and key == \"event\": # end of an earthquake event\n",
    "            rows.append(row)\n",
    "            path.pop()\n",
    "        else:\n",
    "            if event == \"start\" and key not in exclude_tags:\n",
    "                path.append(key)\n",
    "            elif event == \"end\" and key not in exclude_tags and \"event\" in path: # needs to be an event in path, no metadata\n",
    "                current_path = \"/\".join(path)\n",
    "                row[current_path] = element.text\n",
    "                path.pop()    \n",
    "    return rows\n",
    "\n",
    "def get_quakeml(startTime, endTime, queue):\n",
    "    dfList = []\n",
    "    session = requests.Session()\n",
    "    adapter = requests.adapters.HTTPAdapter(pool_maxsize=100) \n",
    "    session.mount('https://', adapter)\n",
    "    parameters = {\"format\":\"quakeml\", \"starttime\":startTime, \"endtime\":endTime, \"limit\":20000, \"minmagnitude\":0, 'orderby':'time-asc', 'eventtype':'earthquake'}\n",
    "    start = perf_counter()\n",
    "    response = session.get(\"https://earthquake.usgs.gov/fdsnws/event/1/query\", params=parameters)\n",
    "    print(f\"timer {perf_counter()-start} {queue.qsize()} {startTime}\")\n",
    "    if response.status_code == 204:\n",
    "        pass\n",
    "    elif response.status_code == 503: # too much, split\n",
    "        timeList = split_time(parameters[\"starttime\"], parameters[\"endtime\"], timedelta(days=365/12), 3)\n",
    "        for i in range(len(timeList)-1):\n",
    "            queue.put((timeList[i], timeList[i+1]))\n",
    "    else:\n",
    "        rows = process_quakeml(response.content)\n",
    "        print(startTime)\n",
    "        datetime.strptime(rows[-1][\"event/origin/time/value\"], \"%Y-%m-%dT%H:%M:%S.%fZ\") # time of youngest row (earthquake): start of new query  \n",
    "        dfList.append(pd.DataFrame(rows))\n",
    "        if len(rows) == 20000 and parameters[\"starttime\"] < parameters[\"endtime\"]: # max records reached\n",
    "            queue.put((datetime.strptime(rows[-1][\"event/origin/time/value\"], \"%Y-%m-%dT%H:%M:%S.%fZ\"), endTime))\n",
    "    \n",
    "    if len(dfList) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return pd.concat(dfList, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b327747-8288-4b51-922a-ccec9d09ca68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_json(features):\n",
    "    rows = []        \n",
    "    for index in range(len(features)):\n",
    "        earthquake = features[index]\n",
    "        prop = earthquake['properties']\n",
    "        coor = earthquake['geometry']['coordinates']\n",
    "        rows.append([earthquake['id'],coor[0],coor[1],coor[2],prop['mag'],prop['place'],prop['time'],prop['updated'],prop['tz'],prop['url'],\n",
    "                     prop['detail'],prop['felt'],prop['cdi'],prop['mmi'],prop['alert'],prop['status'],prop['tsunami'],prop['sig'],prop['net'],\n",
    "                     prop['code'],prop['ids'],prop['sources'],prop['types'],prop['nst'],prop['dmin'],prop['rms'],prop['gap'],prop['magType'],\n",
    "                     prop['type'],prop['title']])\n",
    "        \n",
    "    return pd.DataFrame(rows, columns=['id','longitude','latitude','depth','mag','place','time','updated','tz','url','detail','felt','cdi',\n",
    "                                                  'mmi','alert','status','tsunami','sig','net','code','ids','sources','types','nst','dmin','rms',\n",
    "                                                  'gap','magType','title','type'])\n",
    "def get_json(startTime, endTime):\n",
    "    dfList = []\n",
    "    session = requests.Session()\n",
    "    adapter = requests.adapters.HTTPAdapter(pool_maxsize=250) \n",
    "    session.mount('https://', adapter)\n",
    "    parameters = {\"format\":\"geojson\", \"starttime\":startTime, \"endtime\":endTime, \"limit\":20000, \"minmagnitude\":0, 'orderby':'time-asc', 'eventtype':'earthquake'}\n",
    "\n",
    "    while parameters[\"starttime\"] <= endTime:\n",
    "        response = session.get(\"https://earthquake.usgs.gov/fdsnws/event/1/query\", params=parameters)\n",
    "        if response.status_code == 503:\n",
    "            timeList = split_time(parameters[\"starttime\"], endTime, timedelta(days=365/6), 6) \n",
    "                \n",
    "            for i in range(len(timeList)-1): \n",
    "                parameters[\"starttime\"] = timeList[i]\n",
    "                parameters[\"endtime\"] = timeList[i+1]\n",
    "                response = session.get(\"https://earthquake.usgs.gov/fdsnws/event/1/query\", params=parameters)\n",
    "                features = response.json()['features']\n",
    "                dfList.append(process_json(features))\n",
    "            break # went through time list; end query\n",
    "        else:\n",
    "            features = response.json()['features']\n",
    "            if len(features) == 0: # no results for query; combine dfList\n",
    "                break\n",
    "            parameters[\"starttime\"] = datetime.utcfromtimestamp(features[-1]['properties']['time']/1000) # USGS used UTC timezone    \n",
    "            dfList.append(process_json(features))\n",
    "            if len(features) < 20000: # the query has reached its last earthquake\n",
    "                break\n",
    "\n",
    "    if len(dfList) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return pd.concat(dfList, axis=0, ignore_index=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7616be-c350-494a-ae24-4718c5f8b632",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 101\n",
      "not empty: 100\n",
      "threadpool\n",
      "timer 0.6653590271016583 10 1568-01-01 00:00:00\n",
      "1568-01-01 00:00:00\n",
      "timer 1.1628396379528567 0 1702-03-02 16:05:45.833333\n",
      "1702-03-02 16:05:45.833333\n",
      "timer 1.4852371379965916 0 1900-09-12 15:25:52.054631\n",
      "timer 1.4998168379534036 0 1815-11-11 15:43:22.073705\n",
      "1815-11-11 15:43:22.073705\n",
      "1900-09-12 15:25:52.054631\n",
      "timer 1.7220964550506324 0 1606-01-14 02:00:00\n",
      "1606-01-14 02:00:00\n",
      "timer 1.7158757949946448 0 1889-06-12 10:17:18.605052\n",
      "1889-06-12 10:17:18.605052\n",
      "timer 1.838196607073769 0 1833-03-30 14:24:45.234230\n",
      "1833-03-30 14:24:45.234230\n",
      "timer 1.884523094049655 0 1729-01-09 02:45:17.013889\n",
      "1729-01-09 02:45:17.013889\n",
      "timer 1.9680766159435734 0 1863-10-14 08:16:38.009318\n",
      "1863-10-14 08:16:38.009318\n",
      "timer 2.057446461985819 0 1796-11-23 12:47:18.625860\n",
      "1796-11-23 12:47:18.625860\n",
      "timer 2.081022365950048 0 1640-11-26 09:50:00\n",
      "1640-11-26 09:50:00\n",
      "timer 2.191749162040651 0 1877-03-04 15:35:14.841875\n",
      "1877-03-04 15:35:14.841875\n",
      "timer 2.348964371951297 0 1911-01-05 10:08:42.716745\n",
      "1911-01-05 10:08:42.716745\n",
      "timer 2.6227834949968383 0 1920-06-19 07:17:59.157016\n",
      "1920-06-19 07:17:59.157016\n",
      "timer 3.0824156689923257 0 1849-03-06 11:12:41.464711\n",
      "1849-03-06 11:12:41.464711\n",
      "timer 3.2216395729919896 0 1944-05-10 10:09:01.642355\n",
      "1944-05-10 10:09:01.642355\n",
      "timer 4.076465154998004 0 1951-01-12 09:18:16.505492\n",
      "1951-01-12 09:18:16.505492\n",
      "timer 5.962198485038243 0 1957-02-24 04:31:45.130034\n",
      "timer 6.206483380985446 0 1672-11-12 11:00:50\n",
      "1672-11-12 11:00:50\n",
      "timer 6.662909773993306 0 1937-01-27 19:48:01.791660\n",
      "1937-01-27 19:48:01.791660\n",
      "1957-02-24 04:31:45.130034\n",
      "timer 7.6193486580159515 0 1929-02-17 10:41:29.227265\n",
      "1929-02-17 10:41:29.227265\n",
      "timer 8.664260633056983 0 1962-10-04 18:09:06.369198\n",
      "1962-10-04 18:09:06.369198\n",
      "timer 14.08360647992231 0 2024-05-06 17:02:55.167660\n",
      "2024-05-06 17:02:55.167660\n",
      "timer 16.7327745630173 0 1967-11-25 14:38:20.838432\n",
      "timer 18.486044997931458 0 2024-04-24 23:47:33.711281\n",
      "2024-04-24 23:47:33.711281\n",
      "timer 19.759062553057447 0 2024-03-28 04:32:14.454833\n"
     ]
    }
   ],
   "source": [
    "# taken from https://www.geeksforgeeks.org/python-divide-date-range-to-n-equal-duration/\n",
    "# https://stackoverflow.com/a/29721341\n",
    "def split_time(startTime, endTime, minDelta, divSegment):\n",
    "    diff = endTime - startTime\n",
    "    timeList = [startTime]\n",
    "    while diff > minDelta: # arbitrary (split more recent data)\n",
    "        segment = diff / divSegment # arbitrary division by 6\n",
    "        diff = diff - segment \n",
    "        startTime = startTime + segment\n",
    "        timeList.append(startTime)\n",
    "    timeList.append(endTime)\n",
    "    print(\"time %d\" % len(timeList))\n",
    "    return timeList\n",
    "    \n",
    "def get_data():\n",
    "    start = perf_counter()\n",
    "    startTime, endTime = datetime(1568, 1, 1, 0, 0, 0), datetime(2024, 6, 15) # 1568 get first record (quakeml)\n",
    "    timeList = split_time(startTime, endTime, timedelta(days=365/12), 12)\n",
    "    quakeList, jsonList, quakeFutures = [], [], []\n",
    "    quakeQueue, jsonQueue = Queue(), Queue()\n",
    "\n",
    "    for i in range(len(timeList)-1):\n",
    "        quakeQueue.put((timeList[i],timeList[i+1]))\n",
    "        jsonQueue.put((timeList[i],timeList[i+1]))\n",
    "\n",
    "    while not quakeQueue.empty(): \n",
    "        print(\"not empty: %d\" % quakeQueue.qsize())\n",
    "        executor = ThreadPoolExecutor(max_workers=100)\n",
    "        print(\"threadpool\")\n",
    "        for i in range(quakeQueue.qsize()):\n",
    "            parameters = quakeQueue.get()\n",
    "            quakeFutures.append(executor.submit(get_quakeml, parameters[0], parameters[1], quakeQueue))\n",
    "            \n",
    "            #jsonList = executor.map(get_json, timeList[:-1], timeList[1:])\n",
    "        \n",
    "    wait(quakeFutures, return_when=ALL_COMPLETED)\n",
    "\n",
    "    quakeList = [future.result() for future in quakeFutures]\n",
    "    quakeDf = pd.concat(quakeList, ignore_index=True)\n",
    "    #jsonDf = pd.concat(jsonList, ignore_index=True)\n",
    "    \n",
    "    #print(len(quakeDf.index))\n",
    "    #jsonDf.drop_duplicates(inplace=True) # dates do overlap although miniscule\n",
    "    #quakeDf.drop_duplicates(inplace=True)\n",
    "    #print(len(quakeDf.index))\n",
    "    #jsonDf.reset_index(drop=True, inplace = True)\n",
    "    #quakeDf.reset_index(inplace=True)\n",
    "    print(f\"timer {perf_counter()-start}\")\n",
    "    return quakeDf\n",
    "    '''\n",
    "    # These are all NaN's\n",
    "    df.drop(\"felt\", axis=1, inplace=True)\n",
    "    df.drop(\"cdi\", axis=1, inplace=True) # max intensity (dyfi)\n",
    "    df.drop(\"mmi\", axis=1, inplace=True) # max instrumental intensity (shakemap)\n",
    "    df.drop(\"alert\", axis=1, inplace=True) # not useful\n",
    "    df.drop(\"type\", axis=1, inplace=True) # redundant\n",
    "    df.drop(\"place\", axis=1, inplace=True) # no need to have a reference point when long and lat are provided\n",
    "    df.dropna(inplace=Tp rowrue) # drop all rows with a missing value\n",
    "    \n",
    "    for i, r in df.iterrows(): # accurate reviewed data\n",
    "        if r[\"status\"] == \"automatic\" or r[\"status\"] == \"deleted\":\n",
    "            df.drop(index=i, inplace=True)\n",
    "    df.drop(\"status\", axis=1, inplace=True) # redundant\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"]) # convert object to datetime\n",
    "    \n",
    "    netlocmag_identical = True\n",
    "    for i, r in df.iterrows(): # check for redundancy in columns\n",
    "        if r[\"locationSource\"] != r[\"magSource\"] or r[\"locationSource\"] != r[\"net\"]:\n",
    "            netlocmag_identical = False\n",
    "    if netlocmag_identical: # rename the column to combine and drop the others\n",
    "        df.rename(columns={\"net\": \"netlocmagSource\"}, inplace=True) \n",
    "        df.drop(\"magSource\", axis=1, inplace=True)\n",
    "        df.drop(\"locationSource\", axis=1, inplace=True)'''\n",
    "hi = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d393f866-42fc-4e90-b485-7900ad7e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hi)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
